% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/layers.R
\name{layer_dense_flipout}
\alias{layer_dense_flipout}
\title{Densely-connected layer class with Flipout estimator.}
\usage{
layer_dense_flipout(
  object,
  units,
  activation = NULL,
  activity_regularizer = NULL,
  trainable = TRUE,
  kernel_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(),
  kernel_posterior_tensor_fn = function(d) d \%>\% tfd_sample(),
  kernel_prior_fn = tfp$layers$util$default_multivariate_normal_fn,
  kernel_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  bias_posterior_fn = tfp$layers$util$default_mean_field_normal_fn(is_singular = TRUE),
  bias_posterior_tensor_fn = function(d) d \%>\% tfd_sample(),
  bias_prior_fn = NULL,
  bias_divergence_fn = function(q, p, ignore) tfd_kl_divergence(q, p),
  seed = NULL,
  ...
)
}
\arguments{
\item{object}{What to compose the new \code{Layer} instance with. Typically a
Sequential model or a Tensor (e.g., as returned by \code{layer_input()}).
The return value depends on \code{object}. If \code{object} is:
\itemize{
\item missing or \code{NULL}, the \code{Layer} instance is returned.
\item a \code{Sequential} model, the model with an additional layer is returned.
\item a Tensor, the output tensor from \code{layer_instance(object)} is returned.
}}

\item{units}{integer dimensionality of the output space}

\item{activation}{Activation function. Set it to None to maintain a linear activation.}

\item{activity_regularizer}{Regularizer function for the output.}

\item{trainable}{Whether the layer weights will be updated during training.}

\item{kernel_posterior_fn}{Function which creates \code{tfd$Distribution} instance representing the surrogate
posterior of the \code{kernel} parameter. Default value: \code{default_mean_field_normal_fn()}.}

\item{kernel_posterior_tensor_fn}{Function which takes a \code{tfd$Distribution} instance and returns a representative
value. Default value: \code{function(d) d \%>\% tfd_sample()}.}

\item{kernel_prior_fn}{Function which creates \code{tfd$Distribution} instance. See \code{default_mean_field_normal_fn} docstring for required
parameter signature. Default value: \code{tfd_normal(loc = 0, scale = 1)}.}

\item{kernel_divergence_fn}{Function which takes the surrogate posterior distribution, prior distribution and random variate
sample(s) from the surrogate posterior and computes or approximates the KL divergence. The
distributions are \code{tfd$Distribution}-like instances and the sample is a \code{Tensor}.}

\item{bias_posterior_fn}{Function which creates a \code{tfd$Distribution} instance representing the surrogate
posterior of the \code{bias} parameter. Default value:  \code{default_mean_field_normal_fn(is_singular = TRUE)} (which creates an
instance of \code{tfd_deterministic}).}

\item{bias_posterior_tensor_fn}{Function which takes a \code{tfd$Distribution} instance and returns a representative
value. Default value: \code{function(d) d \%>\% tfd_sample()}.}

\item{bias_prior_fn}{Function which creates \code{tfd} instance. See \code{default_mean_field_normal_fn} docstring for required parameter
signature. Default value: \code{NULL} (no prior, no variational inference)}

\item{bias_divergence_fn}{Function which takes the surrogate posterior distribution, prior distribution and random variate sample(s)
from the surrogate posterior and computes or approximates the KL divergence. The
distributions are \code{tfd$Distribution}-like instances and the sample is a \code{Tensor}.}

\item{seed}{scalar \code{integer} which initializes the random number generator.
Default value: \code{NULL} (i.e., use global seed).}

\item{...}{Additional keyword arguments passed to the \code{keras::layer_dense} constructed by this layer.}
}
\value{
a Keras layer
}
\description{
This layer implements the Bayesian variational inference analogue to
a dense layer by assuming the \code{kernel} and/or the \code{bias} are drawn
from distributions.
}
\details{
By default, the layer implements a stochastic
forward pass via sampling from the kernel and bias posteriors,

\if{html}{\out{<div class="sourceCode">}}\preformatted{kernel, bias ~ posterior
outputs = activation(matmul(inputs, kernel) + bias)
}\if{html}{\out{</div>}}

It uses the Flipout estimator (Wen et al., 2018), which performs a Monte
Carlo approximation of the distribution integrating over the \code{kernel} and
\code{bias}. Flipout uses roughly twice as many floating point operations as the
reparameterization estimator but has the advantage of significantly lower
variance.

The arguments permit separate specification of the surrogate posterior
(\code{q(W|x)}), prior (\code{p(W)}), and divergence for both the \code{kernel} and \code{bias}
distributions.

Upon being built, this layer adds losses (accessible via the \code{losses}
property) representing the divergences of \code{kernel} and/or \code{bias} surrogate
posteriors and their respective priors. When doing minibatch stochastic
optimization, make sure to scale this loss such that it is applied just once
per epoch (e.g. if \code{kl} is the sum of \code{losses} for each element of the batch,
you should pass \code{kl / num_examples_per_epoch} to your optimizer).
}
\section{References}{

\itemize{
\item \href{https://arxiv.org/abs/1803.04386}{Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse. Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches. In \emph{International Conference on Learning Representations}, 2018.}
}
}

\seealso{
Other layers: 
\code{\link{layer_autoregressive}()},
\code{\link{layer_conv_1d_flipout}()},
\code{\link{layer_conv_1d_reparameterization}()},
\code{\link{layer_conv_2d_flipout}()},
\code{\link{layer_conv_2d_reparameterization}()},
\code{\link{layer_conv_3d_flipout}()},
\code{\link{layer_conv_3d_reparameterization}()},
\code{\link{layer_dense_local_reparameterization}()},
\code{\link{layer_dense_reparameterization}()},
\code{\link{layer_dense_variational}()},
\code{\link{layer_variable}()}
}
\concept{layers}
