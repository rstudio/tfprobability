<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Monte-Carlo approximation of an f-Divergence variational loss — vi_monte_carlo_variational_loss • tfprobability</title>

<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
<!-- Bootstrap -->
<link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.3.7/flatly/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous" />

<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script>

<!-- sticky kit -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>



<meta property="og:title" content="Monte-Carlo approximation of an f-Divergence variational loss — vi_monte_carlo_variational_loss" />

<meta property="og:description" content="Variational losses measure the divergence between an unnormalized target
distribution p (provided via target_log_prob_fn) and a surrogate
distribution q (provided as surrogate_posterior). When the
target distribution is an unnormalized posterior from conditioning a model on
data, minimizing the loss with respect to the parameters of
surrogate_posterior performs approximate posterior inference." />
<meta name="twitter:card" content="summary" />



<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->


  </head>

  <body>
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">tfprobability</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.7.0.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/dynamic_linear_models.html">Dynamic linear models</a>
    </li>
    <li>
      <a href="../articles/hamiltonian_monte_carlo.html">Multi-level modeling with Hamiltonian Monte Carlo</a>
    </li>
    <li>
      <a href="../articles/layer_dense_variational.html">Uncertainty estimates with layer_dense_variational</a>
    </li>
  </ul>
</li>
      </ul>
      
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      
      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Monte-Carlo approximation of an f-Divergence variational loss</h1>
    
    <div class="hidden name"><code>vi_monte_carlo_variational_loss.Rd</code></div>
    </div>

    <div class="ref-description">
    
    <p>Variational losses measure the divergence between an unnormalized target
distribution <code>p</code> (provided via <code>target_log_prob_fn</code>) and a surrogate
distribution <code>q</code> (provided as <code>surrogate_posterior</code>). When the
target distribution is an unnormalized posterior from conditioning a model on
data, minimizing the loss with respect to the parameters of
<code>surrogate_posterior</code> performs approximate posterior inference.</p>
    
    </div>

    <pre class="usage"><span class='fu'>vi_monte_carlo_variational_loss</span>(<span class='no'>target_log_prob_fn</span>, <span class='no'>surrogate_posterior</span>,
  <span class='kw'>sample_size</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>discrepancy_fn</span> <span class='kw'>=</span> <span class='no'>vi_kl_reverse</span>,
  <span class='kw'>use_reparametrization</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>seed</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='kw'>name</span> <span class='kw'>=</span> <span class='kw'>NULL</span>)</pre>
    
    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a>Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>target_log_prob_fn</th>
      <td><p>function that takes a set of <code>Tensor</code> arguments
and returns a <code>Tensor</code> log-density. Given
<code>q_sample &lt;- surrogate_posterior$sample(sample_size)</code>, this
will be (in Python) called as <code>target_log_prob_fn(q_sample)</code> if <code>q_sample</code> is a list
or a tuple, <code>target_log_prob_fn(**q_sample)</code> if <code>q_sample</code> is a
dictionary, or <code>target_log_prob_fn(q_sample)</code> if <code>q_sample</code> is a <code>Tensor</code>.
It should support batched evaluation, i.e., should return a result of
shape <code>[sample_size]</code>.</p></td>
    </tr>
    <tr>
      <th>surrogate_posterior</th>
      <td><p>A <code>tfp$distributions$Distribution</code>
instance defining a variational posterior (could be a
<code>tfp$distributions$JointDistribution</code>). Crucially, the distribution's <code>log_prob</code> and
(if reparameterized) <code>sample</code> methods must directly invoke all ops
that generate gradients to the underlying variables. One way to ensure
this is to use <code>tfp$util$DeferredTensor</code> to represent any parameters
defined as transformations of unconstrained variables, so that the
transformations execute at runtime instead of at distribution creation.</p></td>
    </tr>
    <tr>
      <th>sample_size</th>
      <td><p><code>integer</code> number of Monte Carlo samples to use
in estimating the variational divergence. Larger values may stabilize
the optimization, but at higher cost per step in time and memory.
Default value: <code>1</code>.</p></td>
    </tr>
    <tr>
      <th>discrepancy_fn</th>
      <td><p>function representing a Csiszar <code>f</code> function in
in log-space. That is, <code>discrepancy_fn(log(u)) = f(u)</code>, where <code>f</code> is
convex in <code>u</code>.  Default value: <code>vi_kl_reverse</code>.</p></td>
    </tr>
    <tr>
      <th>use_reparametrization</th>
      <td><p><code>logical</code>. When <code>NULL</code> (the default),
automatically set to: <code>surrogate_posterior.reparameterization_type == tfp$distributions$FULLY_REPARAMETERIZED</code>.
When <code>TRUE</code> uses the standard Monte-Carlo average. When <code>FALSE</code> uses the score-gradient trick. (See above for
details.)  When <code>FALSE</code>, consider using <code>csiszar_vimco</code>.</p></td>
    </tr>
    <tr>
      <th>seed</th>
      <td><p><code>integer</code> seed for <code>surrogate_posterior$sample</code>.</p></td>
    </tr>
    <tr>
      <th>name</th>
      <td><p>name prefixed to Ops created by this function.</p></td>
    </tr>
    </table>
    
    <h2 class="hasAnchor" id="value"><a class="anchor" href="#value"></a>Value</h2>

    <p>monte_carlo_variational_loss <code>float</code>-like <code>Tensor</code> Monte Carlo
approximation of the Csiszar f-Divergence.</p>
    
    <h2 class="hasAnchor" id="details"><a class="anchor" href="#details"></a>Details</h2>

    <p>This function defines divergences of the form
<code>E_q[discrepancy_fn(log p(z) - log q(z))]</code>, sometimes known as
<a href='https://en.wikipedia.org/wiki/F-divergence'>f-divergences</a>.</p>
<p>In the special case <code>discrepancy_fn(logu) == -logu</code> (the default
<code>vi_kl_reverse</code>), this is the reverse Kullback-Liebler divergence
<code>KL[q||p]</code>, whose negation applied to an unnormalized <code>p</code> is the widely-used
evidence lower bound (ELBO). Other cases of interest available under
<code>tfp$vi</code> include the forward <code>KL[p||q]</code> (given by <code>vi_kl_forward(logu) == exp(logu) * logu</code>),
total variation distance, Amari alpha-divergences, and more.</p>
<p>Csiszar f-divergences</p>
<p>A Csiszar function <code>f</code> is a convex function from <code>R^+</code> (the positive reals)
to <code>R</code>. The Csiszar f-Divergence is given by:</p><pre>D_f[p(X), q(X)] := E_{q(X)}[ f( p(X) / q(X) ) ]
~= m**-1 sum_j^m f( p(x_j) / q(x_j) ),
where x_j ~iid q(X)
</pre>
    <p>For example, <code>f = lambda u: -log(u)</code> recovers <code>KL[q||p]</code>, while <code>f = lambda u: u * log(u)</code>
recovers the forward <code>KL[p||q]</code>. These and other functions are available in <code>tfp$vi</code>.</p>
<p>Tricks: Reparameterization and Score-Gradient</p>
<p>When q is "reparameterized", i.e., a diffeomorphic transformation of a
parameterless distribution (e.g., <code>Normal(Y; m, s) &lt;=&gt; Y = sX + m, X ~ Normal(0,1)</code>),
we can swap gradient and expectation, i.e.,
<code>grad[Avg{ s_i : i=1...n }] = Avg{ grad[s_i] : i=1...n }</code> where <code>S_n=Avg{s_i}</code>
and <code>s_i = f(x_i), x_i ~iid q(X)</code>.</p>
<p>However, if q is not reparameterized, TensorFlow's gradient will be incorrect
since the chain-rule stops at samples of unreparameterized distributions. In
this circumstance using the Score-Gradient trick results in an unbiased
gradient, i.e.,</p><pre>grad[ E_q[f(X)] ]
  = grad[ int dx q(x) f(x) ]
  = int dx grad[ q(x) f(x) ]
  = int dx [ q'(x) f(x) + q(x) f'(x) ]
  = int dx q(x) [q'(x) / q(x) f(x) + f'(x) ]
  = int dx q(x) grad[ f(x) q(x) / stop_grad[q(x)] ]
  = E_q[ grad[ f(x) q(x) / stop_grad[q(x)] ] ]
</pre>
    <p>Unless <code>q.reparameterization_type != tfd.FULLY_REPARAMETERIZED</code> it is
usually preferable to set <code>use_reparametrization = True</code>.</p>
<p>Example Application:
The Csiszar f-Divergence is a useful framework for variational inference.
I.e., observe that,</p><pre>f(p(x)) =  f( E_{q(Z | x)}[ p(x, Z) / q(Z | x) ] )
        &lt;= E_{q(Z | x)}[ f( p(x, Z) / q(Z | x) ) ]
        := D_f[p(x, Z), q(Z | x)]
</pre>
    <p>The inequality follows from the fact that the "perspective" of <code>f</code>, i.e.,
<code>(s, t) |-&gt; t f(s / t))</code>, is convex in <code>(s, t)</code> when <code>s/t in domain(f)</code> and
<code>t</code> is a real. Since the above framework includes the popular Evidence Lower
BOund (ELBO) as a special case, i.e., <code>f(u) = -log(u)</code>, we call this framework
"Evidence Divergence Bound Optimization" (EDBO).</p>
    
    <h2 class="hasAnchor" id="references"><a class="anchor" href="#references"></a>References</h2>

    
    <ul>
<li><p>Ali, Syed Mumtaz, and Samuel D. Silvey. "A general class of coefficients of divergence of one distribution from another."
Journal of the Royal Statistical Society: Series B (Methodological) 28.1 (1966): 131-142.</p></li>
</ul>
    
    <h2 class="hasAnchor" id="see-also"><a class="anchor" href="#see-also"></a>See also</h2>

    <div class='dont-index'><p>Other vi-functions: <code><a href='vi_amari_alpha.html'>vi_amari_alpha</a></code>,
  <code><a href='vi_kl_forward.html'>vi_kl_forward</a></code>, <code><a href='vi_kl_reverse.html'>vi_kl_reverse</a></code></p></div>
    

  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <h2>Contents</h2>
    <ul class="nav nav-pills nav-stacked">
      <li><a href="#arguments">Arguments</a></li>
      
      <li><a href="#value">Value</a></li>

      <li><a href="#details">Details</a></li>

      <li><a href="#references">References</a></li>

      <li><a href="#see-also">See also</a></li>
          </ul>

  </div>
</div>

      <footer>
      <div class="copyright">
  <p>Developed by Sigrid Keydana.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.3.0.</p>
</div>
      </footer>
   </div>

  

  </body>
</html>

