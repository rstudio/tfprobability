<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>VectorDiffeomixture distribution — tfd_vector_diffeomixture • tfprobability</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<!-- Bootstrap -->
<link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/flatly/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous" />


<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script>

<!-- bootstrap-toc -->
<link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="VectorDiffeomixture distribution — tfd_vector_diffeomixture" />
<meta property="og:description" content="A vector diffeomixture (VDM) is a distribution parameterized by a convex
combination of K component loc vectors, loc[k], k = 0,...,K-1, and K
scale matrices scale[k], k = 0,..., K-1.  It approximates the following
compound distribution
p(x) = int p(x | z) p(z) dz, where z is in the K-simplex, and
p(x | z) := p(x | loc=sum_k z[k] loc[k], scale=sum_k z[k] scale[k])" />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body data-spy="scroll" data-target="#toc">
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">tfprobability</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.12.0.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/dynamic_linear_models.html">Dynamic linear models</a>
    </li>
    <li>
      <a href="../articles/hamiltonian_monte_carlo.html">Multi-level modeling with Hamiltonian Monte Carlo</a>
    </li>
    <li>
      <a href="../articles/layer_dense_variational.html">Uncertainty estimates with layer_dense_variational</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/rstudio/tfprobability/">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>VectorDiffeomixture distribution</h1>
    <small class="dont-index">Source: <a href='https://github.com/rstudio/tfprobability/blob/master/R/distributions.R'><code>R/distributions.R</code></a></small>
    <div class="hidden name"><code>tfd_vector_diffeomixture.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>A vector diffeomixture (VDM) is a distribution parameterized by a convex
combination of <code>K</code> component <code>loc</code> vectors, <code>loc[k], k = 0,...,K-1</code>, and <code>K</code>
<code>scale</code> matrices <code>scale[k], k = 0,..., K-1</code>.  It approximates the following
<a href='https://en.wikipedia.org/wiki/Compound_probability_distribution'>compound distribution</a>
<code>p(x) = int p(x | z) p(z) dz</code>, where z is in the K-simplex, and
<code>p(x | z) := p(x | loc=sum_k z[k] loc[k], scale=sum_k z[k] scale[k])</code></p>
    </div>

    <pre class="usage"><span class='fu'>tfd_vector_diffeomixture</span><span class='op'>(</span>
  <span class='va'>mix_loc</span>,
  <span class='va'>temperature</span>,
  <span class='va'>distribution</span>,
  loc <span class='op'>=</span> <span class='cn'>NULL</span>,
  scale <span class='op'>=</span> <span class='cn'>NULL</span>,
  quadrature_size <span class='op'>=</span> <span class='fl'>8</span>,
  quadrature_fn <span class='op'>=</span> <span class='va'>tfp</span><span class='op'>$</span><span class='va'>distributions</span><span class='op'>$</span><span class='va'>quadrature_scheme_softmaxnormal_quantiles</span>,
  validate_args <span class='op'>=</span> <span class='cn'>FALSE</span>,
  allow_nan_stats <span class='op'>=</span> <span class='cn'>TRUE</span>,
  name <span class='op'>=</span> <span class='st'>"VectorDiffeomixture"</span>
<span class='op'>)</span></pre>

    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a>Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>mix_loc</th>
      <td><p><code>float</code>-like <code>Tensor</code> with shape <code>[b1, ..., bB, K-1]</code>.
In terms of samples, larger <code>mix_loc[..., k]</code> ==&gt;
<code>Z</code> is more likely to put more weight on its <code>kth</code> component.</p></td>
    </tr>
    <tr>
      <th>temperature</th>
      <td><p><code>float</code>-like <code>Tensor</code>. Broadcastable with <code>mix_loc</code>.
In terms of samples, smaller <code>temperature</code> means one component is more
likely to dominate.  I.e., smaller <code>temperature</code> makes the VDM look more
like a standard mixture of <code>K</code> components.</p></td>
    </tr>
    <tr>
      <th>distribution</th>
      <td><p><code>tfp$distributions$Distribution</code>-like instance. Distribution
from which <code>d</code> iid samples are used as input to the selected affine
transformation. Must be a scalar-batch, scalar-event distribution.
Typically <code>distribution$reparameterization_type = FULLY_REPARAMETERIZED</code>
or it is a function of non-trainable parameters. WARNING: If you
backprop through a VectorDiffeomixture sample and the <code>distribution</code>
is not <code>FULLY_REPARAMETERIZED</code> yet is a function of trainable variables,
then the gradient will be incorrect!</p></td>
    </tr>
    <tr>
      <th>loc</th>
      <td><p>Length-<code>K</code> list of <code>float</code>-type <code>Tensor</code>s. The <code>k</code>-th element
represents the <code>shift</code> used for the <code>k</code>-th affine transformation.  If
the <code>k</code>-th item is <code>NULL</code>, <code>loc</code> is implicitly <code>0</code>.  When specified,
must have shape <code>[B1, ..., Bb, d]</code> where <code>b &gt;= 0</code> and <code>d</code> is the event
size.</p></td>
    </tr>
    <tr>
      <th>scale</th>
      <td><p>Length-<code>K</code> list of <code>LinearOperator</code>s. Each should be
positive-definite and operate on a <code>d</code>-dimensional vector space. The
<code>k</code>-th element represents the <code>scale</code> used for the <code>k</code>-th affine
transformation. <code>LinearOperator</code>s must have shape <code>[B1, ..., Bb, d, d]</code>,
<code>b &gt;= 0</code>, i.e., characterizes <code>b</code>-batches of <code>d x d</code> matrices</p></td>
    </tr>
    <tr>
      <th>quadrature_size</th>
      <td><p><code>integer</code> scalar representing number of
quadrature points.  Larger <code>quadrature_size</code> means <code>q_N(x)</code> better
approximates <code>p(x)</code>.</p></td>
    </tr>
    <tr>
      <th>quadrature_fn</th>
      <td><p>Function taking <code>normal_loc</code>, <code>normal_scale</code>,
<code>quadrature_size</code>, <code>validate_args</code> and returning <code><a href='https://rdrr.io/pkg/reticulate/man/tuple.html'>tuple(grid, probs)</a></code>
representing the SoftmaxNormal grid and corresponding normalized weight.
normalized) weight.
Default value: <code>quadrature_scheme_softmaxnormal_quantiles</code>.</p></td>
    </tr>
    <tr>
      <th>validate_args</th>
      <td><p>Logical, default FALSE. When TRUE distribution parameters are checked
for validity despite possibly degrading runtime performance. When FALSE invalid inputs may
silently render incorrect outputs. Default value: FALSE.</p></td>
    </tr>
    <tr>
      <th>allow_nan_stats</th>
      <td><p>Logical, default TRUE. When TRUE, statistics (e.g., mean, mode, variance)
use the value NaN to indicate the result is undefined. When FALSE, an exception is raised if
one or more of the statistic's batch members are undefined.</p></td>
    </tr>
    <tr>
      <th>name</th>
      <td><p>name prefixed to Ops created by this class.</p></td>
    </tr>
    </table>

    <h2 class="hasAnchor" id="value"><a class="anchor" href="#value"></a>Value</h2>

    <p>a distribution instance.</p>
    <h2 class="hasAnchor" id="details"><a class="anchor" href="#details"></a>Details</h2>

    <p>The integral <code>int p(x | z) p(z) dz</code> is approximated with a quadrature scheme
adapted to the mixture density <code>p(z)</code>.  The <code>N</code> quadrature points <code>z_{N, n}</code>
and weights <code>w_{N, n}</code> (which are non-negative and sum to 1) are chosen such that
<code>q_N(x) := sum_{n=1}^N w_{n, N} p(x | z_{N, n}) --&gt; p(x)</code> as <code>N --&gt; infinity</code>.</p>
<p>Since <code>q_N(x)</code> is in fact a mixture (of <code>N</code> points), we may sample from
<code>q_N</code> exactly.  It is important to note that the VDM is <em>defined</em> as <code>q_N</code>
above, and <em>not</em> <code>p(x)</code>.  Therefore, sampling and pdf may be implemented as
exact (up to floating point error) methods.</p>
<p>A common choice for the conditional <code>p(x | z)</code> is a multivariate Normal.
The implemented marginal <code>p(z)</code> is the <code>SoftmaxNormal</code>, which is a
<code>K-1</code> dimensional Normal transformed by a <code>SoftmaxCentered</code> bijector, making
it a density on the <code>K</code>-simplex.  That is,
<code>Z = SoftmaxCentered(X)</code>, <code>X = Normal(mix_loc / temperature, 1 / temperature)</code></p>
<p>The default quadrature scheme chooses <code>z_{N, n}</code> as <code>N</code> midpoints of
the quantiles of <code>p(z)</code> (generalized quantiles if <code>K &gt; 2</code>).
See Dillon and Langmore (2018) for more details.</p>
<p>About <code>Vector</code> distributions in TensorFlow.</p>
<p>The <code>VectorDiffeomixture</code> is a non-standard distribution that has properties
particularly useful in <a href='https://en.wikipedia.org/wiki/Variational_Bayesian_methods'>variational Bayesian methods</a>.
Conditioned on a draw from the SoftmaxNormal, <code>X|z</code> is a vector whose
components are linear combinations of affine transformations, thus is itself
an affine transformation.</p>
<p>Note: The marginals <code>X_1|v, ..., X_d|v</code> are <em>not</em> generally identical to some
parameterization of <code>distribution</code>.  This is due to the fact that the sum of
draws from <code>distribution</code> are not generally itself the same <code>distribution</code>.</p>
<p>About <code>Diffeomixture</code>s and reparameterization.</p>
<p>The <code>VectorDiffeomixture</code> is designed to be reparameterized, i.e., its
parameters are only used to transform samples from a distribution which has no
trainable parameters. This property is important because backprop stops at
sources of stochasticity. That is, as long as the parameters are used <em>after</em>
the underlying source of stochasticity, the computed gradient is accurate.
Reparametrization means that we can use gradient-descent (via backprop) to
optimize Monte-Carlo objectives. Such objectives are a finite-sample
approximation of an expectation and arise throughout scientific computing.</p>
<p>WARNING: If you backprop through a VectorDiffeomixture sample and the "base"
distribution is both: not <code>FULLY_REPARAMETERIZED</code> and a function of trainable
variables, then the gradient is not guaranteed correct!</p>
    <h2 class="hasAnchor" id="references"><a class="anchor" href="#references"></a>References</h2>

    

<ul>
<li><p><a href='https://arxiv.org/abs/1801.03080'>Joshua Dillon and Ian Langmore. Quadrature Compound: An approximating family of distributions. <em>arXiv preprint arXiv:1801.03080</em>, 2018.</a></p></li>
</ul>

    <h2 class="hasAnchor" id="see-also"><a class="anchor" href="#see-also"></a>See also</h2>

    <div class='dont-index'><p>For usage examples see e.g. <code><a href='tfd_sample.html'>tfd_sample()</a></code>, <code><a href='tfd_log_prob.html'>tfd_log_prob()</a></code>, <code><a href='tfd_mean.html'>tfd_mean()</a></code>.</p>
<p>Other distributions: 
<code><a href='tfd_autoregressive.html'>tfd_autoregressive</a>()</code>,
<code><a href='tfd_batch_reshape.html'>tfd_batch_reshape</a>()</code>,
<code><a href='tfd_bates.html'>tfd_bates</a>()</code>,
<code><a href='tfd_bernoulli.html'>tfd_bernoulli</a>()</code>,
<code><a href='tfd_beta_binomial.html'>tfd_beta_binomial</a>()</code>,
<code><a href='tfd_beta.html'>tfd_beta</a>()</code>,
<code><a href='tfd_binomial.html'>tfd_binomial</a>()</code>,
<code><a href='tfd_categorical.html'>tfd_categorical</a>()</code>,
<code><a href='tfd_cauchy.html'>tfd_cauchy</a>()</code>,
<code><a href='tfd_chi2.html'>tfd_chi2</a>()</code>,
<code><a href='tfd_chi.html'>tfd_chi</a>()</code>,
<code><a href='tfd_cholesky_lkj.html'>tfd_cholesky_lkj</a>()</code>,
<code><a href='tfd_continuous_bernoulli.html'>tfd_continuous_bernoulli</a>()</code>,
<code><a href='tfd_deterministic.html'>tfd_deterministic</a>()</code>,
<code><a href='tfd_dirichlet_multinomial.html'>tfd_dirichlet_multinomial</a>()</code>,
<code><a href='tfd_dirichlet.html'>tfd_dirichlet</a>()</code>,
<code><a href='tfd_empirical.html'>tfd_empirical</a>()</code>,
<code><a href='tfd_exp_gamma.html'>tfd_exp_gamma</a>()</code>,
<code><a href='tfd_exp_inverse_gamma.html'>tfd_exp_inverse_gamma</a>()</code>,
<code><a href='tfd_exponential.html'>tfd_exponential</a>()</code>,
<code><a href='tfd_gamma_gamma.html'>tfd_gamma_gamma</a>()</code>,
<code><a href='tfd_gamma.html'>tfd_gamma</a>()</code>,
<code><a href='tfd_gaussian_process_regression_model.html'>tfd_gaussian_process_regression_model</a>()</code>,
<code><a href='tfd_gaussian_process.html'>tfd_gaussian_process</a>()</code>,
<code><a href='tfd_generalized_normal.html'>tfd_generalized_normal</a>()</code>,
<code><a href='tfd_geometric.html'>tfd_geometric</a>()</code>,
<code><a href='tfd_gumbel.html'>tfd_gumbel</a>()</code>,
<code><a href='tfd_half_cauchy.html'>tfd_half_cauchy</a>()</code>,
<code><a href='tfd_half_normal.html'>tfd_half_normal</a>()</code>,
<code><a href='tfd_hidden_markov_model.html'>tfd_hidden_markov_model</a>()</code>,
<code><a href='tfd_horseshoe.html'>tfd_horseshoe</a>()</code>,
<code><a href='tfd_independent.html'>tfd_independent</a>()</code>,
<code><a href='tfd_inverse_gamma.html'>tfd_inverse_gamma</a>()</code>,
<code><a href='tfd_inverse_gaussian.html'>tfd_inverse_gaussian</a>()</code>,
<code><a href='tfd_johnson_s_u.html'>tfd_johnson_s_u</a>()</code>,
<code><a href='tfd_joint_distribution_named_auto_batched.html'>tfd_joint_distribution_named_auto_batched</a>()</code>,
<code><a href='tfd_joint_distribution_named.html'>tfd_joint_distribution_named</a>()</code>,
<code><a href='tfd_joint_distribution_sequential_auto_batched.html'>tfd_joint_distribution_sequential_auto_batched</a>()</code>,
<code><a href='tfd_joint_distribution_sequential.html'>tfd_joint_distribution_sequential</a>()</code>,
<code><a href='tfd_kumaraswamy.html'>tfd_kumaraswamy</a>()</code>,
<code><a href='tfd_laplace.html'>tfd_laplace</a>()</code>,
<code><a href='tfd_linear_gaussian_state_space_model.html'>tfd_linear_gaussian_state_space_model</a>()</code>,
<code><a href='tfd_lkj.html'>tfd_lkj</a>()</code>,
<code><a href='tfd_log_logistic.html'>tfd_log_logistic</a>()</code>,
<code><a href='tfd_log_normal.html'>tfd_log_normal</a>()</code>,
<code><a href='tfd_logistic.html'>tfd_logistic</a>()</code>,
<code><a href='tfd_mixture_same_family.html'>tfd_mixture_same_family</a>()</code>,
<code><a href='tfd_mixture.html'>tfd_mixture</a>()</code>,
<code><a href='tfd_multinomial.html'>tfd_multinomial</a>()</code>,
<code><a href='tfd_multivariate_normal_diag_plus_low_rank.html'>tfd_multivariate_normal_diag_plus_low_rank</a>()</code>,
<code><a href='tfd_multivariate_normal_diag.html'>tfd_multivariate_normal_diag</a>()</code>,
<code><a href='tfd_multivariate_normal_full_covariance.html'>tfd_multivariate_normal_full_covariance</a>()</code>,
<code><a href='tfd_multivariate_normal_linear_operator.html'>tfd_multivariate_normal_linear_operator</a>()</code>,
<code><a href='tfd_multivariate_normal_tri_l.html'>tfd_multivariate_normal_tri_l</a>()</code>,
<code><a href='tfd_multivariate_student_t_linear_operator.html'>tfd_multivariate_student_t_linear_operator</a>()</code>,
<code><a href='tfd_negative_binomial.html'>tfd_negative_binomial</a>()</code>,
<code><a href='tfd_normal.html'>tfd_normal</a>()</code>,
<code><a href='tfd_one_hot_categorical.html'>tfd_one_hot_categorical</a>()</code>,
<code><a href='tfd_pareto.html'>tfd_pareto</a>()</code>,
<code><a href='tfd_pixel_cnn.html'>tfd_pixel_cnn</a>()</code>,
<code><a href='tfd_poisson_log_normal_quadrature_compound.html'>tfd_poisson_log_normal_quadrature_compound</a>()</code>,
<code><a href='tfd_poisson.html'>tfd_poisson</a>()</code>,
<code><a href='tfd_power_spherical.html'>tfd_power_spherical</a>()</code>,
<code><a href='tfd_probit_bernoulli.html'>tfd_probit_bernoulli</a>()</code>,
<code><a href='tfd_quantized.html'>tfd_quantized</a>()</code>,
<code><a href='tfd_relaxed_bernoulli.html'>tfd_relaxed_bernoulli</a>()</code>,
<code><a href='tfd_relaxed_one_hot_categorical.html'>tfd_relaxed_one_hot_categorical</a>()</code>,
<code><a href='tfd_sample_distribution.html'>tfd_sample_distribution</a>()</code>,
<code><a href='tfd_sinh_arcsinh.html'>tfd_sinh_arcsinh</a>()</code>,
<code><a href='tfd_skellam.html'>tfd_skellam</a>()</code>,
<code><a href='tfd_spherical_uniform.html'>tfd_spherical_uniform</a>()</code>,
<code><a href='tfd_student_t_process.html'>tfd_student_t_process</a>()</code>,
<code><a href='tfd_student_t.html'>tfd_student_t</a>()</code>,
<code><a href='tfd_transformed_distribution.html'>tfd_transformed_distribution</a>()</code>,
<code><a href='tfd_triangular.html'>tfd_triangular</a>()</code>,
<code><a href='tfd_truncated_cauchy.html'>tfd_truncated_cauchy</a>()</code>,
<code><a href='tfd_truncated_normal.html'>tfd_truncated_normal</a>()</code>,
<code><a href='tfd_uniform.html'>tfd_uniform</a>()</code>,
<code><a href='tfd_variational_gaussian_process.html'>tfd_variational_gaussian_process</a>()</code>,
<code><a href='tfd_vector_exponential_diag.html'>tfd_vector_exponential_diag</a>()</code>,
<code><a href='tfd_vector_exponential_linear_operator.html'>tfd_vector_exponential_linear_operator</a>()</code>,
<code><a href='tfd_vector_laplace_diag.html'>tfd_vector_laplace_diag</a>()</code>,
<code><a href='tfd_vector_laplace_linear_operator.html'>tfd_vector_laplace_linear_operator</a>()</code>,
<code><a href='tfd_vector_sinh_arcsinh_diag.html'>tfd_vector_sinh_arcsinh_diag</a>()</code>,
<code><a href='tfd_von_mises_fisher.html'>tfd_von_mises_fisher</a>()</code>,
<code><a href='tfd_von_mises.html'>tfd_von_mises</a>()</code>,
<code><a href='tfd_weibull.html'>tfd_weibull</a>()</code>,
<code><a href='tfd_wishart_linear_operator.html'>tfd_wishart_linear_operator</a>()</code>,
<code><a href='tfd_wishart_tri_l.html'>tfd_wishart_tri_l</a>()</code>,
<code><a href='tfd_wishart.html'>tfd_wishart</a>()</code>,
<code><a href='tfd_zipf.html'>tfd_zipf</a>()</code></p></div>

  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top">
      <h2 data-toc-skip>Contents</h2>
    </nav>
  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Sigrid Keydana.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


