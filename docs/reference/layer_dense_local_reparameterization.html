<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Densely-connected layer class with local reparameterization estimator. — layer_dense_local_reparameterization • tfprobability</title>


<!-- jquery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<!-- Bootstrap -->
<link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/flatly/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous" />


<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script>

<!-- bootstrap-toc -->
<link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script>

<!-- Font Awesome icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous" />

<!-- clipboard.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script>

<!-- headroom.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script>

<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script>




<meta property="og:title" content="Densely-connected layer class with local reparameterization estimator. — layer_dense_local_reparameterization" />
<meta property="og:description" content="This layer implements the Bayesian variational inference analogue to
a dense layer by assuming the kernel and/or the bias are drawn
from distributions." />




<!-- mathjax -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->



  </head>

  <body data-spy="scroll" data-target="#toc">
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">tfprobability</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.12.0.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/dynamic_linear_models.html">Dynamic linear models</a>
    </li>
    <li>
      <a href="../articles/hamiltonian_monte_carlo.html">Multi-level modeling with Hamiltonian Monte Carlo</a>
    </li>
    <li>
      <a href="../articles/layer_dense_variational.html">Uncertainty estimates with layer_dense_variational</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/rstudio/tfprobability/">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header>

<div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Densely-connected layer class with local reparameterization estimator.</h1>
    <small class="dont-index">Source: <a href='https://github.com/rstudio/tfprobability/blob/master/R/layers.R'><code>R/layers.R</code></a></small>
    <div class="hidden name"><code>layer_dense_local_reparameterization.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>This layer implements the Bayesian variational inference analogue to
a dense layer by assuming the <code>kernel</code> and/or the <code>bias</code> are drawn
from distributions.</p>
    </div>

    <pre class="usage"><span class='fu'>layer_dense_local_reparameterization</span><span class='op'>(</span>
  <span class='va'>object</span>,
  <span class='va'>units</span>,
  activation <span class='op'>=</span> <span class='cn'>NULL</span>,
  activity_regularizer <span class='op'>=</span> <span class='cn'>NULL</span>,
  trainable <span class='op'>=</span> <span class='cn'>TRUE</span>,
  kernel_posterior_fn <span class='op'>=</span> <span class='va'>tfp</span><span class='op'>$</span><span class='va'>layers</span><span class='op'>$</span><span class='va'>util</span><span class='op'>$</span><span class='fu'>default_mean_field_normal_fn</span><span class='op'>(</span><span class='op'>)</span>,
  kernel_posterior_tensor_fn <span class='op'>=</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>d</span><span class='op'>)</span> <span class='va'>d</span> <span class='op'>%&gt;%</span> <span class='fu'><a href='tfd_sample.html'>tfd_sample</a></span><span class='op'>(</span><span class='op'>)</span>,
  kernel_prior_fn <span class='op'>=</span> <span class='va'>tfp</span><span class='op'>$</span><span class='va'>layers</span><span class='op'>$</span><span class='va'>util</span><span class='op'>$</span><span class='va'>default_multivariate_normal_fn</span>,
  kernel_divergence_fn <span class='op'>=</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>q</span>, <span class='va'>p</span>, <span class='va'>ignore</span><span class='op'>)</span> <span class='fu'><a href='tfd_kl_divergence.html'>tfd_kl_divergence</a></span><span class='op'>(</span><span class='va'>q</span>, <span class='va'>p</span><span class='op'>)</span>,
  bias_posterior_fn <span class='op'>=</span> <span class='va'>tfp</span><span class='op'>$</span><span class='va'>layers</span><span class='op'>$</span><span class='va'>util</span><span class='op'>$</span><span class='fu'>default_mean_field_normal_fn</span><span class='op'>(</span>is_singular <span class='op'>=</span> <span class='cn'>TRUE</span><span class='op'>)</span>,
  bias_posterior_tensor_fn <span class='op'>=</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>d</span><span class='op'>)</span> <span class='va'>d</span> <span class='op'>%&gt;%</span> <span class='fu'><a href='tfd_sample.html'>tfd_sample</a></span><span class='op'>(</span><span class='op'>)</span>,
  bias_prior_fn <span class='op'>=</span> <span class='cn'>NULL</span>,
  bias_divergence_fn <span class='op'>=</span> <span class='kw'>function</span><span class='op'>(</span><span class='va'>q</span>, <span class='va'>p</span>, <span class='va'>ignore</span><span class='op'>)</span> <span class='fu'><a href='tfd_kl_divergence.html'>tfd_kl_divergence</a></span><span class='op'>(</span><span class='va'>q</span>, <span class='va'>p</span><span class='op'>)</span>,
  <span class='va'>...</span>
<span class='op'>)</span></pre>

    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a>Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>object</th>
      <td><p>Model or layer object</p></td>
    </tr>
    <tr>
      <th>units</th>
      <td><p>integer dimensionality of the output space</p></td>
    </tr>
    <tr>
      <th>activation</th>
      <td><p>Activation function. Set it to None to maintain a linear activation.</p></td>
    </tr>
    <tr>
      <th>activity_regularizer</th>
      <td><p>Regularizer function for the output.</p></td>
    </tr>
    <tr>
      <th>trainable</th>
      <td><p>Whether the layer weights will be updated during training.</p></td>
    </tr>
    <tr>
      <th>kernel_posterior_fn</th>
      <td><p>Function which creates <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>kernel</code> parameter. Default value: <code>default_mean_field_normal_fn()</code>.</p></td>
    </tr>
    <tr>
      <th>kernel_posterior_tensor_fn</th>
      <td><p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p></td>
    </tr>
    <tr>
      <th>kernel_prior_fn</th>
      <td><p>Function which creates <code>tfd$Distribution</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required
parameter signature. Default value: <code><a href='tfd_normal.html'>tfd_normal(loc = 0, scale = 1)</a></code>.</p></td>
    </tr>
    <tr>
      <th>kernel_divergence_fn</th>
      <td><p>Function which takes the surrogate posterior distribution, prior distribution and random variate
sample(s) from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p></td>
    </tr>
    <tr>
      <th>bias_posterior_fn</th>
      <td><p>Function which creates a <code>tfd$Distribution</code> instance representing the surrogate
posterior of the <code>bias</code> parameter. Default value:  <code>default_mean_field_normal_fn(is_singular = TRUE)</code> (which creates an
instance of <code>tfd_deterministic</code>).</p></td>
    </tr>
    <tr>
      <th>bias_posterior_tensor_fn</th>
      <td><p>Function which takes a <code>tfd$Distribution</code> instance and returns a representative
value. Default value: <code>function(d) d %&gt;% tfd_sample()</code>.</p></td>
    </tr>
    <tr>
      <th>bias_prior_fn</th>
      <td><p>Function which creates <code>tfd</code> instance. See <code>default_mean_field_normal_fn</code> docstring for required parameter
signature. Default value: <code>NULL</code> (no prior, no variational inference)</p></td>
    </tr>
    <tr>
      <th>bias_divergence_fn</th>
      <td><p>Function which takes the surrogate posterior distribution, prior distribution and random variate sample(s)
from the surrogate posterior and computes or approximates the KL divergence. The
distributions are <code>tfd$Distribution</code>-like instances and the sample is a <code>Tensor</code>.</p></td>
    </tr>
    <tr>
      <th>...</th>
      <td><p>Additional keyword arguments passed to the <code><a href='https://rdrr.io/pkg/keras/man/layer_dense.html'>keras::layer_dense</a></code> constructed by this layer.</p></td>
    </tr>
    </table>

    <h2 class="hasAnchor" id="value"><a class="anchor" href="#value"></a>Value</h2>

    <p>a Keras layer</p>
    <h2 class="hasAnchor" id="details"><a class="anchor" href="#details"></a>Details</h2>

    <p>By default, the layer implements a stochastic
forward pass via sampling from the kernel and bias posteriors,</p><pre>kernel, bias ~ posterior
outputs = activation(matmul(inputs, kernel) + bias)
</pre>

<p>It uses the local reparameterization estimator (Kingma et al., 2015),
which performs a Monte Carlo approximation of the distribution on the hidden
units induced by the <code>kernel</code> and <code>bias</code>. The default <code>kernel_posterior_fn</code>
is a normal distribution which factorizes across all elements of the weight
matrix and bias vector. Unlike that paper's multiplicative parameterization, this
distribution has trainable location and scale parameters which is known as
an additive noise parameterization (Molchanov et al., 2017).</p>
<p>The arguments permit separate specification of the surrogate posterior
(<code><a href='https://rdrr.io/r/base/quit.html'>q(W|x)</a></code>), prior (<code>p(W)</code>), and divergence for both the <code>kernel</code> and <code>bias</code>
distributions.</p>
<p>Upon being built, this layer adds losses (accessible via the <code>losses</code>
property) representing the divergences of <code>kernel</code> and/or <code>bias</code> surrogate
posteriors and their respective priors. When doing minibatch stochastic
optimization, make sure to scale this loss such that it is applied just once
per epoch (e.g. if <code>kl</code> is the sum of <code>losses</code> for each element of the batch,
you should pass <code>kl / num_examples_per_epoch</code> to your optimizer).
You can access the <code>kernel</code> and/or <code>bias</code> posterior and prior distributions
after the layer is built via the <code>kernel_posterior</code>, <code>kernel_prior</code>,
<code>bias_posterior</code> and <code>bias_prior</code> properties.</p>
    <h2 class="hasAnchor" id="references"><a class="anchor" href="#references"></a>References</h2>

    

<ul>
<li><p><a href='https://arxiv.org/abs/1506.02557'>Diederik Kingma, Tim Salimans, and Max Welling. Variational Dropout and the Local Reparameterization Trick. In <em>Neural Information Processing Systems</em>, 2015.</a></p></li>
<li><p><a href='https://arxiv.org/abs/1701.05369'>Dmitry Molchanov, Arsenii Ashukha, Dmitry Vetrov. Variational Dropout Sparsifies Deep Neural Networks. In <em>International Conference on Machine Learning</em>, 2017.</a></p></li>
</ul>

    <h2 class="hasAnchor" id="see-also"><a class="anchor" href="#see-also"></a>See also</h2>

    <div class='dont-index'><p>Other layers: 
<code><a href='layer_autoregressive.html'>layer_autoregressive</a>()</code>,
<code><a href='layer_conv_1d_flipout.html'>layer_conv_1d_flipout</a>()</code>,
<code><a href='layer_conv_1d_reparameterization.html'>layer_conv_1d_reparameterization</a>()</code>,
<code><a href='layer_conv_2d_flipout.html'>layer_conv_2d_flipout</a>()</code>,
<code><a href='layer_conv_2d_reparameterization.html'>layer_conv_2d_reparameterization</a>()</code>,
<code><a href='layer_conv_3d_flipout.html'>layer_conv_3d_flipout</a>()</code>,
<code><a href='layer_conv_3d_reparameterization.html'>layer_conv_3d_reparameterization</a>()</code>,
<code><a href='layer_dense_flipout.html'>layer_dense_flipout</a>()</code>,
<code><a href='layer_dense_reparameterization.html'>layer_dense_reparameterization</a>()</code>,
<code><a href='layer_dense_variational.html'>layer_dense_variational</a>()</code>,
<code><a href='layer_variable.html'>layer_variable</a>()</code></p></div>

  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top">
      <h2 data-toc-skip>Contents</h2>
    </nav>
  </div>
</div>


      <footer>
      <div class="copyright">
  <p>Developed by Sigrid Keydana.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.6.1.</p>
</div>

      </footer>
   </div>

  


  </body>
</html>


